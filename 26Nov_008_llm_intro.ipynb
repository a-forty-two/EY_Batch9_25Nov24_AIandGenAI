{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529t7LK2G_Dv"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Dhs90vKb9d"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Part 1.1:** Recalling Deep Learning\n",
        "\n",
        "Throughout your learning adventure with deep learning, you have probably optimized a variety of models for tasks like classification and regression. In order, you probably advanced in something like the following:\n",
        "\n",
        "- When you started out, you used **linear and logistic regression** to model and interpret simple linear relationships that associated your inputs with your outputs.\n",
        "- When that wasn't enough, you started **stacking linear layers one after another and adding non-linear activations** to give your model more predictive power.\n",
        "- When your data started getting intractably high-dimensional, you started using more **informed sparsely-connected techniques like convolution** to add more control to your reasoning criteria.\n",
        "- When you realized that you didn't have enough data to properly train your models for each specific task, you got **pre-trained components (i.e. VGG-16/ResNet)** that were trained on a giant repository of training data and already contained the necessary logic you wanted.\n",
        "\n",
        "\n",
        "\n",
        "If you’ve gone through these steps, you already possess the foundational skills to tackle complex topics, including the vast field of language modeling.\n",
        "\n",
        "Like vision, language is highly complex and high-dimensional. For instance, a simple 200x200 color image contains $200\\times 200\\times 3 = 120,000$ features! Now, consider the even larger number of combinations possible in natural language—**it’s enormous!** Fortunately, creative techniques and large pre-trained models simplify the problem, providing efficient solutions.\n",
        "\n",
        "**This course will show you how to approach language modeling, the tools available, and the types of problems you can solve!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSpqwEwG_Dx"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DfWbMaOQxe"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Part 1.2:** Pulling In Our First LLM\n",
        "\n",
        "Rather than building models from scratch, this course focuses on using powerful tools to simplify the process. A great place to start is **HuggingFace** &#x1F917;.\n",
        "\n",
        "[**HuggingFace**](https://huggingface.co/) is an open-source community that offers simple strategies for accessing, developing, and hosting large deep learning models for testing and deployment. The topics they support span many tasks and modalities, but we'll be focusing on large language models (**LLMs**) for most of this course.\n",
        "\n",
        "When searching through the [**HuggingFace Models catalog**](https://huggingface.co/models?sort=downloads&search=bert), you'll quickly stumble upon the [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model. Taking a look at its card, you'll see several interesting things:\n",
        "\n",
        "- **The [`transformers`](https://github.com/huggingface/transformers) Package**: Loading in the model requires the use of the [`transformers`](https://github.com/huggingface/transformers) package, which is HuggingFace’s primary library for language models. Its name refers to the transformer architecture, which we’ll explore further in upcoming sections. You’ll be using `transformers` throughout this course, so feel free to dive into the source code as you go.\n",
        "\n",
        "- **Pipelines**: HuggingFace simplifies complex deep learning tasks with its [Pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines). A pipeline allows you to move from input to output without worrying about the internal workings. We’ll demonstrate this with the `bert-base-uncased` model for mask-filling.\n",
        "\n",
        "As a representative example, we can go ahead and pull in the discussed [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model and test it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ERP_WrTRP2N",
        "outputId": "473ba271-eb7f-4860-c957-08ff07337e9a",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.10731059312820435,\n",
              "  'token': 4827,\n",
              "  'token_str': 'fashion',\n",
              "  'sequence': \"hello i ' m a fashion model.\"},\n",
              " {'score': 0.08774524927139282,\n",
              "  'token': 2535,\n",
              "  'token_str': 'role',\n",
              "  'sequence': \"hello i ' m a role model.\"},\n",
              " {'score': 0.05338389053940773,\n",
              "  'token': 2047,\n",
              "  'token_str': 'new',\n",
              "  'sequence': \"hello i ' m a new model.\"},\n",
              " {'score': 0.046672068536281586,\n",
              "  'token': 3565,\n",
              "  'token_str': 'super',\n",
              "  'sequence': \"hello i ' m a super model.\"},\n",
              " {'score': 0.02709585428237915,\n",
              "  'token': 2986,\n",
              "  'token_str': 'fine',\n",
              "  'sequence': \"hello i ' m a fine model.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "## Loading in the pipeline and predict the mask fill (example from model card)\n",
        "unmasker = pipeline(\n",
        "    'fill-mask',\n",
        "    model='bert-base-uncased',\n",
        "    device='cuda',  ## Feel free to use GPU. For such a small model, not necessary\n",
        ")\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfUHbutJO4Nr"
      },
      "source": [
        "**And just like that, it works!** You have just obtained an intuitive answer that makes sense with human logic, which makes you almost forget that there is actually a deep learning model calculating probabilities to make this all work.\n",
        "\n",
        "**But that's what this course is for: to see what's actually going on behind the scenes and know how to use it to make interesting and useful systems.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA9vUG_Ny9Cn"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "## **Part 1.3:** Dissecting The Pipeline\n",
        "\n",
        "While the pipeline provides a clean interface—taking strings in and returning a dictionary—it doesn’t give us much insight into what’s happening under the hood. Let’s peel back a layer and examine the approximate inner workings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atoVpxD7zgFO",
        "outputId": "9f5d0c5a-37aa-415f-b0f6-1ec335782bb2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.23865924775600433,\n",
              "  'token': 2183,\n",
              "  'token_str': 'going',\n",
              "  'sequence': 'hello, mr. bert! how is it going?'},\n",
              " {'score': 0.07178767770528793,\n",
              "  'token': 2017,\n",
              "  'token_str': 'you',\n",
              "  'sequence': 'hello, mr. bert! how is it you?'},\n",
              " {'score': 0.05827939882874489,\n",
              "  'token': 6230,\n",
              "  'token_str': 'happening',\n",
              "  'sequence': 'hello, mr. bert! how is it happening?'},\n",
              " {'score': 0.056334055960178375,\n",
              "  'token': 2651,\n",
              "  'token_str': 'today',\n",
              "  'sequence': 'hello, mr. bert! how is it today?'},\n",
              " {'score': 0.052870072424411774,\n",
              "  'token': 2085,\n",
              "  'token_str': 'now',\n",
              "  'sequence': 'hello, mr. bert! how is it now?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer, BertModel, FillMaskPipeline, AutoModelForMaskedLM, BertForMaskedLM, BertForPreTraining\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel        ## General-purpose fully-automatic\n",
        "from transformers import AutoModelForMaskedLM            ## Default import for FillMaskPipeline\n",
        "from transformers import BertTokenizer, BertForMaskedLM  ## Realized components after automatic resolution\n",
        "\n",
        "class MyMlmPipeline(FillMaskPipeline):\n",
        "    ## My Masked Language Modeling Pipeline\n",
        "\n",
        "    ### CASE 0: Construct your pipeline automatically by pulling in the components\n",
        "    ###   with their respective configs from HuggingFace. Pipeline assumes preprocessing/postprocessing.\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        ## The fully-automatic version\n",
        "        super().__init__(\n",
        "            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
        "            model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n",
        "            # model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\"),\n",
        "            *args, **kwargs  ## <- pass in any extra arguments\n",
        "        )\n",
        "\n",
        "    ### CASE 1: Uncomment out the __call__ method to see what data is flowing.\n",
        "    \"\"\"\n",
        "    def __call__(self, string, verbose=True):\n",
        "        ## Verbose argument just there for our convenience\n",
        "        input_tensors = self.preprocess(string)\n",
        "        if verbose: print('\\npreprocess outputs:\\n', input_tensors, '\\n')\n",
        "        output_tensors = self.forward(input_tensors)\n",
        "        if verbose: print('forward outputs:\\n', output_tensors, '\\n')\n",
        "        output = self.postprocess(output_tensors)\n",
        "        return output\n",
        "    \"\"\"\n",
        "\n",
        "    ### CASE 2: Uncomment out the manual overrides below to verify the pipeline still works\n",
        "    \"\"\"\n",
        "    def preprocess(self, string):\n",
        "        string = [string] if isinstance(string, str) else string\n",
        "        inputs = self.tokenizer(string, return_tensors=\"pt\")           ### strings -> indices\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}          ### move to GPU\n",
        "        return inputs\n",
        "\n",
        "    def forward(self, tensor_dict):\n",
        "        output_tensors = self.model.forward(**tensor_dict)             ### indices -> vectors -> probabilities\n",
        "        return {**output_tensors, **tensor_dict}\n",
        "\n",
        "    def postprocess(self, tensor_dict):\n",
        "        ## Very Task-specific; see FillMaskPipeline.postprocess\n",
        "        tensor_dict = {k: v.to(\"cpu\") for k, v in tensor_dict.items()} ### move off GPU\n",
        "        return super().postprocess(tensor_dict)                        ### probabilities (or vectors) -> outputs\n",
        "    \"\"\"\n",
        "\n",
        "unmasker = MyMlmPipeline(device=\"cuda\")\n",
        "unmasker(\"Hello, Mr. Bert! How is it [MASK]?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLoEmptNztUb"
      },
      "source": [
        "We can also see that the model primarily consists of two core components:\n",
        "- **Tokenizer**: Converts input strings into a format the model can process.\n",
        "- **Model**: The deep learning engine that transforms input tensors into output tensors.\n",
        "\n",
        "Together, these components support the pipeline’s intuitive workflow:\n",
        "- `preprocess`: human-intuitive input $\\to$ tensor inputs. Facilitated by `tokenizer`\n",
        "- `forward`: tensor inputs $\\to$ tensor outputs. Facilitated by `model`\n",
        "- `postprocess`: tensor outputs $\\to$ human-intuitive outputs. Facilitated by the pipeline task.\n",
        "\n",
        "For deep learning, this flow makes sense. The model operates on numbers, but the users are working with words, to this abstraction greatly simplifies the LLM interface for typical scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4zUZZnHG_D0"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsanqWLwG_D1",
        "outputId": "be6e5188-dc00-4993-b31a-a2d271486aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================================\n",
            "GPU SPECIFICATION\n",
            "===================================================\n",
            "Tue Nov 26 11:14:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0              27W /  70W |   1059MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "\n",
            "===================================================\n",
            "MEMORY SPECIFICATION\n",
            "===================================================\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       2.6Gi       5.0Gi        15Mi       5.1Gi       9.8Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "GPU SPECIFICATION\n",
        "===================================================\"\"\"\n",
        "nvidia-smi\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "MEMORY SPECIFICATION\n",
        "===================================================\"\"\"\n",
        "free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP8rsMYTG_D1",
        "outputId": "04479f98-3ca8-4c51-81d0-8f889f829b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tWXI0W8LtZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}